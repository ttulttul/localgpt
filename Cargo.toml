[workspace]
members = ["gen"]
default-members = ["."]
resolver = "3"

[package]
name = "localgpt"
version = "0.2.0"
edition = "2024"
description = "A local device focused AI assistant with persistent markdown memory, autonomous heartbeat tasks, and semantic search. Single binary, no runtime dependencies."
license = "Apache-2.0"
repository = "https://github.com/localgpt-app/localgpt"
homepage = "https://localgpt.app"
documentation = "https://github.com/localgpt-app/localgpt/blob/main/README.md"
readme = "README.md"
keywords = ["ai", "assistant", "llm", "memory", "local"]
categories = ["command-line-utilities", "text-processing"]

[features]
default = ["desktop"]
# Desktop GUI (eframe/egui). Disable for headless/server/Docker builds.
desktop = ["eframe"]
# GGUF embedding model support via llama.cpp (requires C++ compiler)
gguf = ["llama-cpp-2"]
# 3D scene generation via Bevy renderer (LocalGPT Gen)
gen = ["bevy", "image"]

[dependencies]
# Async runtime
tokio = { version = "1.49", features = ["full"] }

# CLI
clap = { version = "4.5", features = ["derive", "env"] }

# HTTP client for LLM APIs
reqwest = { version = "0.13", features = ["json", "stream"] }

# HTTP server
axum = { version = "0.8", features = ["ws", "macros"] }
tower-http = { version = "0.6", features = ["cors", "trace"] }

# Database
rusqlite = { version = "0.38", features = ["bundled", "functions", "vtab", "load_extension"] }

# Vector search extension for SQLite
sqlite-vec = "0.1.7-alpha.2"

# Local embeddings (default - no API key needed)
fastembed = "5.9"

# GGUF embeddings via llama.cpp (optional, requires C++ compiler)
llama-cpp-2 = { version = "0.1", optional = true }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
json5 = "1.3"
toml = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# File watching
notify = "8.2"

# Token counting
tiktoken-rs = "0.9"

# Static file embedding for Web UI
rust-embed = { version = "8", features = ["compression"] }
mime_guess = "2.0"

# Utilities
chrono = { version = "0.4", features = ["serde"] }
directories = "6.0"
etcetera = "0.8"
libc = "0.2"
thiserror = "2.0"
anyhow = "1.0"
uuid = { version = "1.20", features = ["v4"] }
async-trait = "0.1"
futures = "0.3"
tokio-stream = "0.1"
async-stream = "0.3"
shellexpand = "3.1"
glob = "0.3"
base64 = "0.22"
regex = "1"
once_cell = "1"
fs2 = "0.4"
rand = "0.10"

# Security (HMAC signing, hashing)
sha2 = "0.10"
hmac = "0.12"

# CLI line editor
rustyline = "17.0.2"

# Telegram bot
teloxide = { version = "0.17", features = ["macros"] }

# Desktop GUI (optional — disable with --no-default-features for headless builds)
eframe = { version = "0.33", optional = true, default-features = false, features = [
    "default_fonts",
    "glow",
    "persistence",
    "x11",
    "wayland",
] }

# 3D generation via Bevy (optional — enable with --features gen)
bevy = { version = "0.15", optional = true }
image = { version = "0.25", optional = true }

# Unix daemonization and sandbox (process isolation)
[target.'cfg(unix)'.dependencies]
daemonize = "0.5"
nix = { version = "0.31", features = ["process", "resource", "signal"] }

# Linux sandbox (Landlock + seccomp)
[target.'cfg(target_os = "linux")'.dependencies]
landlock = "0.4"
seccompiler = "0.5"

[target.'cfg(target_os = "linux")'.build-dependencies]
cc = "1"

[dev-dependencies]
tempfile = "3.25"
mockall = "0.14"

[[bin]]
name = "localgpt"
path = "src/main.rs"

[profile.release]
lto = true
codegen-units = 1
strip = true
